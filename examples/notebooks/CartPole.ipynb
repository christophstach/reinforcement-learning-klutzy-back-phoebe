{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from stachrl.agents import DQNAgent, DDQNAgent\n",
    "from stachrl.utils import movingaverage\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared parameters of all agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10000\n",
    "timesteps = 500\n",
    "exploration_rate_decay = 0.005\n",
    "memory_size = 50000\n",
    "update_interval = 50\n",
    "\n",
    "\n",
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "env._max_episode_steps = timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [16:51<00:00,  9.89it/s, Reward: 245.80, ε: 0.01]\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(\n",
    "    name=env_name,\n",
    "    state_shape=env.observation_space.shape,\n",
    "    action_size=env.action_space.n,\n",
    "    exploration_rate_decay=exploration_rate_decay,\n",
    "    memory_size=memory_size,\n",
    "    production=False,\n",
    "    auto_save=False,\n",
    "    auto_load=False\n",
    ")\n",
    "\n",
    "total_reward_history_dqn = []\n",
    "iterator = tqdm(range(episodes))\n",
    "\n",
    "for episode in iterator:\n",
    "    iterator.set_postfix_str('Reward: {:.2f}, ε: {:.2f}'.format(\n",
    "        movingaverage(total_reward_history_dqn, 50)[-1],\n",
    "        agent.exploration_rate\n",
    "    ))\n",
    "\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    agent.before()\n",
    "    for timestep in range(timesteps):\n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward = reward - abs(next_state[0])\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward = total_reward + reward\n",
    "\n",
    "        if done:\n",
    "            total_reward_history_dqn.append(total_reward)\n",
    "            break\n",
    "\n",
    "    agent.after()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the DDQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 9926/10000 [15:01<00:13,  5.57it/s, Reward: 352.21, ε: 0.01]"
     ]
    }
   ],
   "source": [
    "agent = DDQNAgent(\n",
    "    name=env_name,\n",
    "    state_shape=env.observation_space.shape,\n",
    "    action_size=env.action_space.n,\n",
    "    exploration_rate_decay=exploration_rate_decay,\n",
    "    memory_size=memory_size,\n",
    "    update_interval=update_interval,\n",
    "    production=False,\n",
    "    auto_save=False,\n",
    "    auto_load=False\n",
    ")\n",
    "\n",
    "total_reward_history_ddqn = []\n",
    "iterator = tqdm(range(episodes))\n",
    "\n",
    "for episode in iterator:\n",
    "    iterator.set_postfix_str('Reward: {:.2f}, ε: {:.2f}'.format(\n",
    "        movingaverage(total_reward_history_ddqn, 50)[-1],\n",
    "        agent.exploration_rate\n",
    "    ))\n",
    "\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    agent.before()\n",
    "    for timestep in range(timesteps):\n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward = reward - abs(next_state[0])\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward = total_reward + reward\n",
    "\n",
    "        if done:\n",
    "            total_reward_history_ddqn.append(total_reward)\n",
    "            break\n",
    "\n",
    "    agent.after()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dqn = movingaverage(total_reward_history_dqn, 50)\n",
    "x_ddqn = movingaverage(total_reward_history_ddqn, 50)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "line_dqn, = plt.plot(x_dqn, label='DQN')\n",
    "line_ddqn, = plt.plot(x_ddqn, label='DDQN') \n",
    "\n",
    "plt.legend(handles=[\n",
    "    line_dqn,\n",
    "    line_ddqn\n",
    "])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
